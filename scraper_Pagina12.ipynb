{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Probando el request y detalles para la página de noticias Página 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://www.pagina12.com.ar/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p12 = requests.get(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p12.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p12.headers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p12.request.headers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p12.request.method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p12.request.url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = BeautifulSoup(p12.text, 'lxml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(s.prettify())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s.find('ul')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s.find('ul', attrs={'class': 'hot-sections'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s.find('div', attrs={'class': 'p12-dropdown-column'}).find('a',attrs={'class': 'p12-dropdown-item'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s.find('ul', attrs={'class':'horizontal-list main-sections hide-on-dropdown'}).find_all('a')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s.find('div', attrs={'class':'p12-dropdown-column'}).find_all('a')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "secciones = s.find('div', attrs={'class':'p12-dropdown-column'}).find_all('a')\n",
    "secciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seccion = secciones[1]\n",
    "seccion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seccion.get_text()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seccion.get('href')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "links_secciones = [seccion.get('href') for seccion in secciones]\n",
    "links_secciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    sec = requests.get(links_secciones[1])\n",
    "    print(sec)\n",
    "except Exception as e:\n",
    "    print(\"Error en el request\")\n",
    "    print(e)\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sec.status_code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s_seccion = BeautifulSoup(sec.text,'lxml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(s_seccion.prettify())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "featured_article = s_seccion.find('div', attrs={'class':'article-item__content'})\n",
    "featured_article"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "featured_article.a.get('href')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "article_list = s_seccion.find('section',attrs={'class':'list-content'}).find_all('h4', attrs={'class':'is-display-inline'})\n",
    "article_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#article_below = article_list[0]\n",
    "article_links = [article_below1.a.get('href') for article_below1 in article_list]\n",
    "article_links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def obtener_notas(soup):\n",
    "    '''Función que recibe un objeto de Beautiful Soup de una página de una sección y devuelve una lista de URLS \n",
    "    a la notas de esa seccion'''\n",
    "    \n",
    "    lista_notas = []\n",
    "    \n",
    "    #Obtengo el artículo promocionado\n",
    "    featured_article = soup.find('div', attrs={'class':'article-item__content'})\n",
    "    if featured_article:\n",
    "        lista_notas.append(featured_article.a.get('href'))\n",
    "        \n",
    "    #Obtengo el listado de artículos\n",
    "    article_list = s_seccion.find('section',attrs={'class':'list-content'}).find_all('h4', attrs={'class':'is-display-inline'})\n",
    "    for i in article_list:\n",
    "        lista_notas.append(i.a.get('href'))\n",
    "    \n",
    "    return lista_notas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lista_notas = (obtener_notas(s_seccion))\n",
    "lista_notas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url_nota = lista_notas[7]\n",
    "url_nota"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for link in lista_notas:\n",
    "    try:\n",
    "        nota = requests.get(link)\n",
    "        if nota.status_code == 200:\n",
    "            s_nota = BeautifulSoup(nota.text,'lxml')\n",
    "            #Extrae titulo\n",
    "            titulo = s_nota.find('h1',attrs={'class':'article-title'})\n",
    "            fecha = s_nota.find('span',attrs={'pubdate':'pubdate'}).get('datetime')\n",
    "            autor = s_nota.find('div',attrs={'class':'article-author'}).find('a').text\n",
    "            descripcion = s_nota.find('h2',attrs={'class':'article-prefix'}).text\n",
    "            resumen = s_nota.find('div',attrs={'class':'article-summary'}).text\n",
    "            cuerpo = s_nota.find('div',attrs={'class':'article-text'}).text\n",
    "            print(titulo.text)\n",
    "            print(fecha)\n",
    "            if autor == None:\n",
    "                print('Articulo sin autor')\n",
    "            else:\n",
    "                print('Autor: ' + autor)\n",
    "            print(descripcion)\n",
    "            print(resumen)\n",
    "            print(cuerpo)\n",
    "            print('----------')\n",
    "    except Exception as e:\n",
    "        print(\"Error:\")\n",
    "        print(e)\n",
    "        print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "        nota = requests.get(link)\n",
    "        if nota.status_code == 200:\n",
    "            s_nota = BeautifulSoup(nota.text,'lxml')\n",
    "            #Extrae titulo\n",
    "            try:\n",
    "                titulo = s_nota.find('h1',attrs={'class':'article-title'})\n",
    "                print(titulo.text)\n",
    "            except Exception as e:\n",
    "                pass\n",
    "            fecha = s_nota.find('span',attrs={'pubdate':'pubdate'}).get('datetime')\n",
    "            print(fecha)\n",
    "            try:\n",
    "                autor = s_nota.find('div',attrs={'class':'article-author'}).find('a').text\n",
    "                if autor == None:\n",
    "                    print('Articulo sin autor')\n",
    "                else:\n",
    "                    print('Autor: ' + autor)\n",
    "            except Exception as e:\n",
    "                pass\n",
    "            try:\n",
    "                descripcion = s_nota.find('h2',attrs={'class':'article-prefix'}).text\n",
    "                print(descripcion)\n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "            try:\n",
    "                resumen = s_nota.find('div',attrs={'class':'article-summary'}).text\n",
    "                if resumen == None:\n",
    "                    print('Articulo sin resumen')\n",
    "                else:\n",
    "                    print(resumen)\n",
    "            except Exception as e:\n",
    "                print('Articulo sin resumen')\n",
    "            cuerpo = s_nota.find('div',attrs={'class':'article-text'}).text\n",
    "            print(cuerpo)\n",
    "            print('--------------------------')\n",
    "except Exception as e:\n",
    "        print(\"Error:\")\n",
    "        print(e)\n",
    "        print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "media = s_nota.find('div',attrs={'article-main-media-image'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imagenes = media.find_all('img')\n",
    "imagenes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(imagenes)==0:\n",
    "    print('no se encontraron imagenes')\n",
    "else:\n",
    "    imagen = imagenes[-1]\n",
    "    img_src = imagen.get('data-src')\n",
    "    print(img_src)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_req = requests.get(img_src)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "img_req.status_code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image(img_req.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Iniciando Scraper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://www.pagina12.com.ar/'\n",
    "p12 = requests.get(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = BeautifulSoup(p12.text, 'lxml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "secciones = s.find('div', attrs={'class':'p12-dropdown-column'}).find_all('a')\n",
    "secciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "links_secciones = [seccion.get('href') for seccion in secciones]\n",
    "links_secciones"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Función para obtener la lista de urls de una sección de noticias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def obtener_notas(soup):\n",
    "    '''Función que recibe un objeto de Beautiful Soup de una página de una sección y devuelve una lista de URLS \n",
    "    a la notas de esa seccion'''\n",
    "    \n",
    "    lista_notas = []\n",
    "    \n",
    "    #Obtengo el artículo promocionado\n",
    "    featured_article = soup.find('div', attrs={'class':'article-item__content'})\n",
    "    if featured_article:\n",
    "        lista_notas.append(featured_article.a.get('href'))\n",
    "        \n",
    "    #Obtengo el listado de artículos\n",
    "    article_list = s_seccion.find('section',attrs={'class':'list-content'}).find_all('h4', attrs={'class':'is-display-inline'})\n",
    "    for i in article_list:\n",
    "        lista_notas.append(i.a.get('href'))\n",
    "    \n",
    "    return lista_notas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Función para obtener los datos de cada noticia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def request_links(s_nota):\n",
    "    #Diccionario vacío para poblar data\n",
    "    rect_dict = {}\n",
    "    \n",
    "    #Extraemos fecha \n",
    "    fecha = s_nota.find('span',attrs={'pubdate':'pubdate'})\n",
    "    if fecha:\n",
    "        rect_dict['fecha'] = fecha.get('datetime')\n",
    "    else:\n",
    "        rect_dict['fecha'] = None \n",
    "    \n",
    "    #Extraemos el título\n",
    "    titulo = s_nota.find('h1',attrs={'class':'article-title'})\n",
    "    if titulo:\n",
    "        rect_dict['titulo'] = titulo.text\n",
    "    else:\n",
    "        rect_dict['titulo'] = None\n",
    "        \n",
    "    #Extraemos el autor \n",
    "    autor = s_nota.find('div',attrs={'class':'article-author'}).find('a')\n",
    "    if autor:\n",
    "        rect_dict['autor'] = autor.text\n",
    "    else:\n",
    "        rect_dict['autor'] = None\n",
    "        \n",
    "    #Extraemos la descripción\n",
    "    descripcion = s_nota.find('h2',attrs={'class':'article-prefix'})\n",
    "    if descripcion:\n",
    "        rect_dict['descripcion'] = descripcion.text\n",
    "    else:\n",
    "        rect_dict['descripcion'] = None\n",
    "    \n",
    "    #Extraemos el resumen\n",
    "    resumen = s_nota.find('div',attrs={'class':'article-summary'})\n",
    "    if resumen:\n",
    "            rect_dict['resumen'] = resumen.text\n",
    "    else:\n",
    "        rect_dict['resumen'] = None\n",
    "    \n",
    "    #Obtenemos la imagen\n",
    "    media = s_nota.find('div',attrs={'article-main-media-image'})\n",
    "    if media:\n",
    "        imagenes = media.find_all('img')\n",
    "        if len(imagenes)==0:\n",
    "            print('no se encontraron imagenes')\n",
    "        else:\n",
    "            imagen = imagenes[-1]\n",
    "            img_src = imagen.get('data-src')\n",
    "            try:\n",
    "                img_req = requests.get(img_src)\n",
    "                if img_req.status_code == 200:\n",
    "                    rect_dict['img'] = img_req.contentt\n",
    "                else:\n",
    "                    rect_dict['img'] = None\n",
    "            except Exception as e:\n",
    "                print('No se pudo obtener la imagen')\n",
    "    else:\n",
    "        print('No se encontró media')\n",
    "    \n",
    "    #Extraemos el cuerpo\n",
    "    cuerpo = s_nota.find('div',attrs={'class':'article-text'})\n",
    "    if descripcion:\n",
    "        rect_dict['cuerpo'] = cuerpo.text\n",
    "    else:\n",
    "        rect_dict['cuerpo'] = None\n",
    "    \n",
    "    return rect_dict    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Función para iniciar el scraping a la url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_nota(url):\n",
    "    try: \n",
    "        nota = requests.get(url)\n",
    "    except Exception as e:\n",
    "        print('Error scrapeando la URL', url)\n",
    "        print(e)\n",
    "        return None\n",
    "    \n",
    "    if nota.status_code != 200:\n",
    "        print(f'Error obteniendo nota {url}')\n",
    "        print(f'Status code = {nota.status_code}')\n",
    "        return None \n",
    "              \n",
    "    s_nota = BeautifulSoup(nota.text, 'lxml')\n",
    "              \n",
    "    rect_dict = request_links(s_nota)\n",
    "    rect_dict['url'] = url \n",
    "              \n",
    "    return rect_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scrape_nota(url_nota)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "links_secciones"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Obtener todos los links a scrapear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "notas = []\n",
    "for link in links_secciones:\n",
    "    try:\n",
    "        r = requests.get(link)\n",
    "        if r.status_code == 200:\n",
    "            soup = BeautifulSoup(r.text,'lxml')\n",
    "            notas.extend(obtener_notas(soup))\n",
    "        else:\n",
    "            print('No se pudo obtener la seccion', link)\n",
    "    except:\n",
    "        print('No se pudo obtener el link ', link)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "notas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comenzar el scraper por cada link obtenido"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "for i, nota in enumerate(notas):\n",
    "    print(f'Scrapeando , nota{i}/{len(notas)}')\n",
    "    data.append(scrape_nota(nota))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mostrando la data obtenida en un dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exportando a un CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('Noticias_pag12.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
